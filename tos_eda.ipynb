{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the normal stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get JSON files from TOS API\n",
    "import urllib, json\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of file names to pull each json file\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp imports, chose TFIDF since some words are repeated\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model imports\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Master DF\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "All of the data for ToS;DR is stored in a directory of different json files, each labeled with a different company name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull out the data \n",
    "companies = [f for f in listdir('tosdr.org/api/1/service') if isfile(join('tosdr.org/api/1/service', f))]\n",
    "\n",
    "#try and find non-english companies to remove, looking at every letter \n",
    "#(not just first index in case non-english word appears later)\n",
    "\n",
    "import string\n",
    "ascii_chars = set(string.printable)\n",
    "nonenglish = {word for word in companies for letter in word if letter not in ascii_chars}\n",
    "nonenglish.remove( 'coinbase–.json')\n",
    "companies = [company for company in companies if company not in nonenglish]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for company in companies:\n",
    "    with open(f'tosdr.org/api/1/service/{company}') as json_data:\n",
    "        data_list.append(json.load(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create main dataframe, drop unnecessary columns, expand list of lists\n",
    "\n",
    "pd_list = []\n",
    "for data in data_list:\n",
    "    pd_list.append(pd.DataFrame.from_dict(data['pointsData']).T.loc[:,['id','quoteText','services','title','tosdr']])\n",
    "\n",
    "alldata_df = pd.concat(pd_list,axis=0).reset_index()\n",
    "todsr_df = json_normalize(alldata_df['tosdr'])\n",
    "df = pd.concat((alldata_df,todsr_df),axis=1)\n",
    "df = df.drop('tosdr',axis=1).explode('services')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create master document column with tldr as base and quote text as secondar\n",
    "\n",
    "df['document'] = df['tldr']\n",
    "df['document'] = np.where(df['document'] == '',df['quoteText'],df['document'])\n",
    "df['document'] = np.where(df['document'] == 'Generated through the annotate view',df['quoteText'],df['document'])\n",
    "df['document'].fillna(df['quoteText'])\n",
    "\n",
    "#if there is no text in the tldr or quoteText column, drop and reset index\n",
    "df.dropna(axis=0,subset=['document'],inplace=True)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create labels, the point is really to distinquish \"bad\" language so labeled \n",
    "#both good and neutral as a success\n",
    "\n",
    "df['label'] = df['point']\n",
    "df.label = df.label.replace(['blocker','bad','neutral','good'],['bad','bad','neutral','good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                          4766\n",
       "id                                                             4766\n",
       "quoteText         Summary</strong>\\n</p>\\n<p>We collect informat...\n",
       "services                                                   kink-com\n",
       "title             The service provides details about what kinds ...\n",
       "binding                                                        True\n",
       "case              The service provides details about what kinds ...\n",
       "point                                                          good\n",
       "privacyRelated                                                  NaN\n",
       "score                                                            30\n",
       "tldr              Generated through the annotate view; updated t...\n",
       "sources                                                         NaN\n",
       "irrelevant                                                      NaN\n",
       "reason                                                          NaN\n",
       "tmp_rating                                                      NaN\n",
       "document          Generated through the annotate view; updated t...\n",
       "label                                                          good\n",
       "Name: 2197, dtype: object"
      ]
     },
     "execution_count": 1053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2197]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteText</th>\n",
       "      <th>tldr</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2197</td>\n",
       "      <td>Summary&lt;/strong&gt;\\n&lt;/p&gt;\\n&lt;p&gt;We collect informat...</td>\n",
       "      <td>Generated through the annotate view; updated t...</td>\n",
       "      <td>Generated through the annotate view; updated t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>\\n&lt;p&gt;Please note that some parts of our Servic...</td>\n",
       "      <td>Generated through the annotate view</td>\n",
       "      <td>\\n&lt;p&gt;Please note that some parts of our Servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1569</td>\n",
       "      <td>0.\\nData Retention &lt;/strong&gt;\\n&lt;/p&gt;\\n&lt;p&gt;We reta...</td>\n",
       "      <td>Generated through the annotate view</td>\n",
       "      <td>0.\\nData Retention &lt;/strong&gt;\\n&lt;/p&gt;\\n&lt;p&gt;We reta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Short and to the point.</td>\n",
       "      <td>Short and to the point.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1277</td>\n",
       "      <td>You are responsible for maintaining the confid...</td>\n",
       "      <td>Generated through the annotate view</td>\n",
       "      <td>You are responsible for maintaining the confid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              quoteText  \\\n",
       "2197  Summary</strong>\\n</p>\\n<p>We collect informat...   \n",
       "42    \\n<p>Please note that some parts of our Servic...   \n",
       "1569  0.\\nData Retention </strong>\\n</p>\\n<p>We reta...   \n",
       "2071                                                NaN   \n",
       "1277  You are responsible for maintaining the confid...   \n",
       "\n",
       "                                                   tldr  \\\n",
       "2197  Generated through the annotate view; updated t...   \n",
       "42                  Generated through the annotate view   \n",
       "1569                Generated through the annotate view   \n",
       "2071                            Short and to the point.   \n",
       "1277                Generated through the annotate view   \n",
       "\n",
       "                                               document  \n",
       "2197  Generated through the annotate view; updated t...  \n",
       "42    \\n<p>Please note that some parts of our Servic...  \n",
       "1569  0.\\nData Retention </strong>\\n</p>\\n<p>We reta...  \n",
       "2071                            Short and to the point.  \n",
       "1277  You are responsible for maintaining the confid...  "
      ]
     },
     "execution_count": 1051,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random lookup to check data\n",
    "\n",
    "df.loc[np.random.randint(1,2564,5),['quoteText','tldr','document']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The terms of service state that \"Signal does not sell, rent or monetize your personal data or content in any way – ever.\"',\n",
       " 'Last updated: June 07, 2019')"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the land of sad documents\n",
    "df.document[0],df.document[722]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Shitty Model \n",
    "### Start taking the parts of the dataframe that I want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create a list of clean companies that can be used as stop words - use \n",
    "springcleaning function to format them in a similar way to the rest of \n",
    "tfiddy\n",
    "'''\n",
    "dirty_companies = [company.split('.') for company in companies]\n",
    "clean_companies = [springcleaning(company[0]) for company in dirty_companies]\n",
    "cleanest_companies = [company for sublist in clean_companies for company in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['signal',\n",
       " 'musik',\n",
       " 'sammler',\n",
       " 'whatismyip',\n",
       " 'com',\n",
       " 'email',\n",
       " 'cz',\n",
       " 'upcloud',\n",
       " 'customink',\n",
       " 'brilliant',\n",
       " 'pure',\n",
       " 'zenimaxmediainc',\n",
       " 'qwant',\n",
       " 'virtbiz',\n",
       " 'any',\n",
       " 'do',\n",
       " 'visible',\n",
       " 'sprint',\n",
       " 'stackoverflow',\n",
       " 'symantec',\n",
       " 'gitlab',\n",
       " 'airbnb',\n",
       " 'websaver',\n",
       " 'mcdonald',\n",
       " 'kitsu',\n",
       " 'meetup',\n",
       " 'weebly',\n",
       " 'pexgle',\n",
       " 'apple',\n",
       " 'web',\n",
       " 'de',\n",
       " 'dr',\n",
       " 'mcdougall',\n",
       " 'shealth',\n",
       " 'medicalcenter',\n",
       " 'newegg',\n",
       " 'com',\n",
       " 'yelp',\n",
       " 'carfax',\n",
       " 'jetbrains',\n",
       " 'diytubevideocommunity',\n",
       " 'windowslogicproductions',\n",
       " 'virgin',\n",
       " 'discogs',\n",
       " 'digitaladvertisingplatform',\n",
       " 'reklamstore',\n",
       " 'cnn',\n",
       " 'forbes',\n",
       " 'npm',\n",
       " 'w3schools',\n",
       " 'etesync',\n",
       " 'librarything',\n",
       " 'wikimedia',\n",
       " 'algolia',\n",
       " 'imdb',\n",
       " 'freecodecamp',\n",
       " 'steam',\n",
       " 'crunchyroll',\n",
       " 'reputation',\n",
       " 'coursehero',\n",
       " 'vox',\n",
       " 'osu',\n",
       " 'nordvpn',\n",
       " 'myspace',\n",
       " 'tellonym',\n",
       " 'goguardian',\n",
       " 'quora',\n",
       " 'lastpass',\n",
       " 'mewe',\n",
       " 'chilliapps',\n",
       " 'abandonmentprotector',\n",
       " 'ancestry',\n",
       " 'pythonanywhere',\n",
       " 'sonic',\n",
       " 'net',\n",
       " 'wikia',\n",
       " 'allrecipes',\n",
       " 'kongregate',\n",
       " 'moddb',\n",
       " 'com',\n",
       " 'amnestyinternationallimited',\n",
       " 'uk',\n",
       " 'lootcrate',\n",
       " 'todoist',\n",
       " 'tiktok',\n",
       " 'cloudsight',\n",
       " 'protonmail',\n",
       " 'riseup',\n",
       " 'net',\n",
       " 'discord',\n",
       " 'furaffinity',\n",
       " 'roblox',\n",
       " 'gog',\n",
       " 'com',\n",
       " 'cityofreading',\n",
       " 'amazon',\n",
       " 'indeed',\n",
       " 'instagram',\n",
       " 'mega',\n",
       " 'researchgate',\n",
       " 'patreon',\n",
       " 'rainforestalliance',\n",
       " 'livearchivistblog',\n",
       " 'bit',\n",
       " 'ly',\n",
       " 'inruptinc',\n",
       " 'bing',\n",
       " 'speaky',\n",
       " 'tvtropes',\n",
       " 'bitchute',\n",
       " 'miicharacters',\n",
       " 'ccleaner',\n",
       " 'ryver',\n",
       " 'heretechnologies',\n",
       " 'here',\n",
       " 'apple',\n",
       " 'icloud',\n",
       " 'paypal',\n",
       " 'com',\n",
       " 'nickwasused',\n",
       " 'com',\n",
       " 'duolingo',\n",
       " 'pornhub',\n",
       " 'vimeo',\n",
       " 'startpage',\n",
       " 'paypal',\n",
       " 'web',\n",
       " 'seodesigners',\n",
       " 'academia',\n",
       " 'edu',\n",
       " 'hackernews',\n",
       " 'mastodon',\n",
       " 'social',\n",
       " 'brainly',\n",
       " 'pixelcatproductionsjbauth',\n",
       " 'dictionary',\n",
       " 'isodme',\n",
       " 'parsec',\n",
       " 'intuitivepassword',\n",
       " 'yase',\n",
       " 'fwg1240',\n",
       " 'dictionary',\n",
       " 'com',\n",
       " 'mailinator',\n",
       " 'stackexchange',\n",
       " 'openweathermap',\n",
       " 'jobvite',\n",
       " 'openstreetmap',\n",
       " 'techsmith',\n",
       " 'screencast',\n",
       " 'com',\n",
       " 'bbc',\n",
       " 'socialblade',\n",
       " 'urban',\n",
       " 'dictionary',\n",
       " '500px',\n",
       " 'huntandjump',\n",
       " 'stablemanaginggame',\n",
       " 'colloq',\n",
       " 'io',\n",
       " 'nationalgeographic',\n",
       " 'skype',\n",
       " 'ifttt',\n",
       " 'internetarchive',\n",
       " 'medium',\n",
       " 'minecraft',\n",
       " 'nanowrimo',\n",
       " 'duckduckgo',\n",
       " 'searchencrypt',\n",
       " 'lichess',\n",
       " 'itch',\n",
       " 'io',\n",
       " 'readthedocscommunity',\n",
       " 'uiiverse',\n",
       " 'grammarly',\n",
       " 'expobeds',\n",
       " 'theguardian',\n",
       " 'cnet',\n",
       " 'foxnews',\n",
       " 'roboform',\n",
       " 'flickr',\n",
       " 'snopes',\n",
       " 'uber',\n",
       " 'twitch',\n",
       " 'spielaffe',\n",
       " 'anki',\n",
       " 'goodreads',\n",
       " 'posteo',\n",
       " 'snapchat',\n",
       " 'skycloud',\n",
       " 'travelzoo',\n",
       " 'brave',\n",
       " 'spotify',\n",
       " 'fotocommunity',\n",
       " 'frontendmasters',\n",
       " 'pepper',\n",
       " 'carrot',\n",
       " 'discovery',\n",
       " 'threemaweb',\n",
       " 'tinder',\n",
       " 'letsencrypt',\n",
       " 'getrentback',\n",
       " 'spideroak',\n",
       " 'dailymotion',\n",
       " 'liamrosenfeld',\n",
       " 'com',\n",
       " 'groupon',\n",
       " 'bitbucket',\n",
       " 'etsy',\n",
       " 'uphold',\n",
       " 'crowdmark',\n",
       " 'shapeshift',\n",
       " 'akamai',\n",
       " 'creditkarma',\n",
       " 'humblebundle',\n",
       " 'youtube',\n",
       " 'jagex',\n",
       " 'fastmail',\n",
       " 'biblegateway',\n",
       " 'tumblr',\n",
       " 'cloudflare',\n",
       " 'couchsurfing',\n",
       " 'opentradestatistics',\n",
       " 'vinted',\n",
       " 'amazonaws',\n",
       " 'toptree',\n",
       " 'cc',\n",
       " 'bumble',\n",
       " 'attainbyaetna',\n",
       " 'wikihow',\n",
       " 'disqus',\n",
       " 'bitsoffreedom',\n",
       " 'osmand',\n",
       " 'arc',\n",
       " 'games',\n",
       " 'shmoop',\n",
       " '1password',\n",
       " 'intercom',\n",
       " 'blendswap',\n",
       " 'icann',\n",
       " 'loom',\n",
       " 'yahoo',\n",
       " 'albine',\n",
       " 'massdrop',\n",
       " 'nabucasa',\n",
       " 'nexon',\n",
       " 'indiegogo',\n",
       " 'pocket',\n",
       " 'epicgames',\n",
       " 'offtopical',\n",
       " 'netpodcast',\n",
       " 'lydia',\n",
       " 'istudiez',\n",
       " 'netflix',\n",
       " 'trello',\n",
       " 'habbo',\n",
       " 'irecommend',\n",
       " 'ru',\n",
       " 'hq',\n",
       " 'zappos',\n",
       " 'shortcutworld',\n",
       " 'com',\n",
       " 'directly',\n",
       " 'cxautomation',\n",
       " 'expertsattheheartofai',\n",
       " 'herbalife',\n",
       " 'lagirafeextraordinaire',\n",
       " 'idka',\n",
       " 'zoosk',\n",
       " '420a3',\n",
       " 'com',\n",
       " 'ikeausa',\n",
       " 'wordpress',\n",
       " 'com',\n",
       " 'khanacademy',\n",
       " 'boingo',\n",
       " 'packvel',\n",
       " 'nytimes',\n",
       " 'newyorktimes',\n",
       " 'grabcraft',\n",
       " 'musicbrainz',\n",
       " 'blogspot',\n",
       " 'expertsattheheartofai',\n",
       " 'repl',\n",
       " 'it',\n",
       " 'ebird',\n",
       " 'freesteamkeys',\n",
       " 'wanikani',\n",
       " 'quake',\n",
       " 'remove',\n",
       " 'bg',\n",
       " 'kjvpce',\n",
       " 'bluestacks',\n",
       " 'overleaf',\n",
       " 'kolabnow',\n",
       " 'avaaz',\n",
       " 'fireapps',\n",
       " 'tapas',\n",
       " 'inaturalist',\n",
       " 'sony',\n",
       " 'whatsapp',\n",
       " 'cloudant',\n",
       " 'tutanota',\n",
       " 'yr',\n",
       " 'freeforums',\n",
       " 'facebook',\n",
       " 'microsoft',\n",
       " 'store',\n",
       " 'ctrlaltdev',\n",
       " 'craigslist',\n",
       " 'betterhelp',\n",
       " 'thumbzilla',\n",
       " 'wikipedia',\n",
       " 'crowdin',\n",
       " 'honeypot',\n",
       " 'spinonline',\n",
       " 'dnd5e',\n",
       " 'info',\n",
       " 'vudu',\n",
       " 'ask',\n",
       " 'jstor',\n",
       " 'pole',\n",
       " 'emotion',\n",
       " 'enpass',\n",
       " 'tunnelbear',\n",
       " 'fiverr',\n",
       " 'devsoap',\n",
       " 'webmd',\n",
       " '10minutemail',\n",
       " 'newgrounds',\n",
       " 'shopify',\n",
       " 'ello',\n",
       " 'bittorrent',\n",
       " 'wizardsofthecoast',\n",
       " 'jonhosting',\n",
       " 'microsoft',\n",
       " 'quizlet',\n",
       " 'cengage',\n",
       " 'blendermarket',\n",
       " 'eyewire',\n",
       " 'nabble',\n",
       " 'spigotmc',\n",
       " 'uswarrantservice',\n",
       " 'sourcehut',\n",
       " 'observable',\n",
       " 'delicious',\n",
       " 'kahoot',\n",
       " 'scratch',\n",
       " 'evaneos',\n",
       " 'thehelloworldcollection',\n",
       " 'wugtodon',\n",
       " 'werk32',\n",
       " 'net',\n",
       " 'rampow',\n",
       " 'harpermacawshop',\n",
       " 'fakku',\n",
       " 'softpedia',\n",
       " 'mysudo',\n",
       " 'dashlane',\n",
       " 'xvideos',\n",
       " 'gravatar',\n",
       " 'faceapp',\n",
       " 'niche',\n",
       " 'coursera',\n",
       " 'packagetrackr',\n",
       " 'dropbox',\n",
       " 'discoursehosting',\n",
       " 'openwrtforum',\n",
       " 'whirlpool',\n",
       " 'yakka',\n",
       " 'mrichard333',\n",
       " 'envato',\n",
       " 'abc',\n",
       " 'australia',\n",
       " 'mozilla',\n",
       " 'org',\n",
       " 'tacticaltech',\n",
       " 'famileo',\n",
       " 'tosdr',\n",
       " 'github',\n",
       " 'witches',\n",
       " 'livemastodon',\n",
       " 'patook',\n",
       " 'twitter',\n",
       " 'mullvad',\n",
       " 'gab',\n",
       " 'netsymstechnologies',\n",
       " 'coinbase',\n",
       " 'hp',\n",
       " '67',\n",
       " 'mailfence',\n",
       " 'campusinnovationskultur',\n",
       " 'ankiweb',\n",
       " 'ironhorsegames',\n",
       " 'evernote',\n",
       " 'ebay',\n",
       " 'slack',\n",
       " 'privacytoolsio',\n",
       " 'flattr',\n",
       " 'wattpad',\n",
       " 'nrc',\n",
       " 'upakka',\n",
       " 'reddit',\n",
       " 'gov',\n",
       " 'uk',\n",
       " 'kink',\n",
       " 'com',\n",
       " 'bitwarden',\n",
       " 'feedly',\n",
       " 'google',\n",
       " 'myanimelist',\n",
       " 'ecosia',\n",
       " 'minds',\n",
       " 'livejournal',\n",
       " 'soundcloud',\n",
       " 'infosechandbook',\n",
       " 'linewebtoon',\n",
       " 'redbubble',\n",
       " 'privacyinternational',\n",
       " 'plexsolutions',\n",
       " 'icepop',\n",
       " 'godaddy',\n",
       " 'imgur',\n",
       " 'royalsocietyofchemistrypublications',\n",
       " 'scrubly',\n",
       " 'pinterest',\n",
       " 'linkedin',\n",
       " 'tartle',\n",
       " 'projectgutenberg',\n",
       " 'setlisting',\n",
       " 'slither',\n",
       " 'io',\n",
       " 'alignable',\n",
       " 'masonbee',\n",
       " 'seenthis',\n",
       " 'speedtestbyookla',\n",
       " 'pixabay',\n",
       " 'roll20',\n",
       " 'strava',\n",
       " 'presearch',\n",
       " 'rabbit',\n",
       " 'openhumans',\n",
       " 'nvidia',\n",
       " 'mailchimp',\n",
       " 'olx',\n",
       " 'wire',\n",
       " 'walmart',\n",
       " 'mint',\n",
       " 'deviantart',\n",
       " 'xing',\n",
       " 'healthline',\n",
       " 'goodtodo',\n",
       " 'huawei',\n",
       " 'billiger',\n",
       " 'de',\n",
       " 'openbookpublishers',\n",
       " 'medtinker',\n",
       " 'sonicdrive',\n",
       " 'in',\n",
       " 'tos',\n",
       " 'dr']"
      ]
     },
     "execution_count": 1055,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanest_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before clean up: \n",
      " The terms of service state: \"You must be at least 13 years old to use our Services. The minimum age to use our Services without parental approval may be higher in your home country.\"\n",
      "\n",
      "After clean up: \n",
      " ['the', 'terms', 'of', 'service', 'state', 'you', 'must', 'be', 'at', 'least', '13', 'years', 'old', 'to', 'use', 'our', 'services', 'the', 'minimum', 'age', 'to', 'use', 'our', 'services', 'without', 'parental', 'approval', 'may', 'be', 'higher', 'in', 'your', 'home', 'country']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''uses a method in tfidf to explore the initial clean up step - lowering \n",
    "case, removing punctuation, and splitting words'''\n",
    "\n",
    "tfiddy = TfidfVectorizer()\n",
    "tfiddy.fit(X_train)\n",
    "springcleaning = tfiddy.build_analyzer()\n",
    "before = df['document'][1]\n",
    "after = springcleaning(df['document'][1])\n",
    "print(f'Before clean up: \\n {before}\\n')\n",
    "print(f'After clean up: \\n {after}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['document']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_good = df[df['point'] == 'good']['document']\n",
    "X_neutral = df[df['point'] == 'neutral']['document']\n",
    "X_bad = df[df['point'] == 'bad']['document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MX_train, MX_test, My_train, My_test = train_test_split(X,multiy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfiddy = TfidfVectorizer()\n",
    "tfiddy = TfidfVectorizer(stop_words=cleanest_companies)\n",
    "tfiddy.fit(X_train)\n",
    "X_train_tfiddy = tfiddy.transform(X_train).toarray()\n",
    "X_test_tfiddy = tfiddy.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countvoncount the countvectorizer\n",
    "voncount = CountVectorizer(stop_words=cleanest_companies)\n",
    "voncount.fit(X_train)\n",
    "X_train_voncount = voncount.transform(X_train).toarray()\n",
    "X_test_voncount = voncount.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2549, 5077)"
      ]
     },
     "execution_count": 1063,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voncount = CountVectorizer(stop_words=cleanest_companies)\n",
    "voncount_all = voncount.fit_transform(df['document']).toarray()\n",
    "voncount_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [],
   "source": [
    "toswords = voncount.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of all words: 100436 \n",
      "Count of words in good documents: 24371\n",
      "Count of words in netural documents: 25654 \n",
      "Count of words in bad documents: 48933\n"
     ]
    }
   ],
   "source": [
    "all_sum = voncount_all.sum()\n",
    "good_sum = voncount_all[df['point'] == 'good'].sum()\n",
    "neutral_sum = voncount_all[df['point'] == 'neutral'].sum()\n",
    "bad_sum = voncount_all[df['point'] == 'bad'].sum()\n",
    "print(f'Count of all words: {all_sum} \\nCount of words in good documents: {good_sum}\\\n",
    "\\nCount of words in netural documents: {neutral_sum} \\nCount of words in bad documents: {bad_sum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_count = np.sum(voncount_all,axis=0)\n",
    "good_count = np.sum(voncount_all[df['point'] =='good'],axis = 0)\n",
    "neutral_count = np.sum(voncount_all[df['point'] =='neutral'],axis = 0)\n",
    "bad_count = np.sum(voncount_all[df['point'] =='bad'],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atruehonor to meet such a distinguished word\n",
    "\n",
    "atruehonor = {}\n",
    "ahonor = {}\n",
    "afalsehonor = {}\n",
    "alph = 1.2\n",
    "\n",
    "for class_word_count,total_word_count,word in zip(good_count,all_count,toswords):\n",
    "    atruehonor[word] = ((class_word_count**alph)/total_word_count)\n",
    "for class_word_count,total_word_count,word in zip(neutral_count,all_count,toswords):\n",
    "    ahonor[word] = ((class_word_count**alph)/total_word_count)\n",
    "for class_word_count,total_word_count,word in zip(bad_count,all_count,toswords):\n",
    "    afalsehonor[word] = ((class_word_count**alph)/total_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('days', 1.7665314390948643),\n",
       " ('want', 1.7427325876521218),\n",
       " ('ownership', 1.6996910167106056),\n",
       " ('la', 1.6952182030724354),\n",
       " ('never', 1.6785540014044966),\n",
       " ('nous', 1.6153942662021779),\n",
       " ('unsubscribe', 1.6113486821132612),\n",
       " ('delete', 1.5907632457820151),\n",
       " ('logging', 1.5848931924611134),\n",
       " ('rent', 1.5848931924611134),\n",
       " ('30', 1.5836862258314204),\n",
       " ('deleted', 1.5588992181955612),\n",
       " ('easy', 1.5518455739153594),\n",
       " ('erase', 1.5518455739153594),\n",
       " ('factor', 1.5518455739153594)]"
      ]
     },
     "execution_count": 1068,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(atruehonor).most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('13', 2.5561596864740617),\n",
       " ('age', 2.319414969825008),\n",
       " ('years', 2.315420595626855),\n",
       " ('2018', 2.2028611071638102),\n",
       " ('2019', 2.1117857649667533),\n",
       " ('responsible', 2.051052907200627),\n",
       " ('old', 2.0188930732512738),\n",
       " ('last', 2.0027726320280808),\n",
       " ('18', 1.9833726264391651),\n",
       " ('jurisdiction', 1.9341084497715753),\n",
       " ('updated', 1.9028694297028423),\n",
       " ('16', 1.8956420888727294),\n",
       " ('older', 1.838416287252544),\n",
       " ('california', 1.8285714285714283),\n",
       " ('children', 1.82056420302608)]"
      ]
     },
     "execution_count": 1069,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ahonor).most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 2.701013004903969),\n",
       " ('or', 2.5074970275430384),\n",
       " ('beacons', 2.409233475252727),\n",
       " ('indemnify', 2.353391971359324),\n",
       " ('harmless', 2.3455876685050026),\n",
       " ('services', 2.297735728363006),\n",
       " ('cookies', 2.281080551985294),\n",
       " ('including', 2.28035047020096),\n",
       " ('costs', 2.244786134364092),\n",
       " ('technologies', 2.2193931289698874),\n",
       " ('damages', 2.216661358042455),\n",
       " ('of', 2.20620205407551),\n",
       " ('claims', 2.1991228900700497),\n",
       " ('expenses', 2.1954018974274896),\n",
       " ('party', 2.1865924439616062)]"
      ]
     },
     "execution_count": 1070,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(afalsehonor).most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18063766, 0.09450074, 0.30470142, ..., 0.10489251, 0.14307376,\n",
       "       0.10509293])"
      ]
     },
     "execution_count": 1071,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tfiddy[X_test_tfiddy != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7931034482758621"
      ]
     },
     "execution_count": 1072,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first shitty model!\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB(alpha=.01)\n",
    "model.fit(X_train_tfiddy, y_train)\n",
    "y_hat = model.predict(X_test_tfiddy)\n",
    "\n",
    "thisisdumb = y_test + y_hat \n",
    "(thisisdumb == 'badgood').sum()\n",
    "(thisisdumb == 'goodbad').sum()\n",
    "model.score(X_test_tfiddy, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7789968652037618"
      ]
     },
     "execution_count": 1073,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count vectorizer instead of TFIDF\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB(alpha=.01)\n",
    "model.fit(X_train_voncount, y_train)\n",
    "y_hat = model.predict(X_test_voncount)\n",
    "\n",
    "model.score(X_test_voncount, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8134796238244514"
      ]
     },
     "execution_count": 1080,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the complement to naive bayes\n",
    "model = ComplementNB(alpha=0.5)\n",
    "model.fit(X_train_tfiddy, y_train)\n",
    "y_hat = model.predict(X_test_tfiddy)\n",
    "\n",
    "model.score(X_test_tfiddy, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 1090,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lostwoods = GradientBoostingClassifier(n_estimators=100)\n",
    "lostwoods.fit(X_train_tfiddy,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {},
   "outputs": [],
   "source": [
    "losthats = lostwoods.predict(X_test_tfiddy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7633228840125392"
      ]
     },
     "execution_count": 1091,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lostwoods.score(X_test_tfiddy,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB(alpha=0.5)\n",
    "model.fit(X_train_tfiddy, y_train)\n",
    "y_hat = model.predict(X_test_tfiddy)\n",
    "\n",
    "thisisdumb = y_test + y_hat \n",
    "(thisisdumb == 'badgood').sum()\n",
    "(thisisdumb == 'goodbad').sum()\n",
    "model.score(X_test_tfiddy, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7931034482758621"
      ]
     },
     "execution_count": 1075,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multinomial classifier\n",
    "\n",
    "tfiddy = TfidfVectorizer(stop_words=cleanest_companies)\n",
    "tfiddy.fit(X_train)\n",
    "X_train_tfiddy = tfiddy.transform(X_train).toarray()\n",
    "X_test_tfiddy = tfiddy.transform(X_test).toarray()\n",
    "\n",
    "model = MultinomialNB(alpha=0.01)\n",
    "model.fit(X_train_tfiddy, y_train)\n",
    "multiy_hat = model.predict(X_test_tfiddy)\n",
    "\n",
    "model.score(X_test_tfiddy, y_test)\n",
    "#model.predict_proba(X_test_tfiddy)\n",
    "#model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7640625"
      ]
     },
     "execution_count": 1076,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multinomial countvectorizer\n",
    "\n",
    "voncount = CountVectorizer(stop_words=cleanest_companies)\n",
    "voncount.fit(MX_train)\n",
    "MX_train_voncount = voncount.transform(MX_train).toarray()\n",
    "MX_test_voncount = voncount.transform(MX_test).toarray()\n",
    "\n",
    "model = MultinomialNB(alpha=0.01)\n",
    "model.fit(MX_train_voncount, My_train)\n",
    "multiy_hat = model.predict(MX_test_voncount)\n",
    "\n",
    "model.score(MX_test_voncount, My_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
